{
	"enableProposedApi": true,
	"repository": "https://github.com/TheFuseGamer/vs-codex",
	"name": "vs-codex",
	"displayName": "VS Codex",
	"description": "OpentAI Codex auto-completion for VS Code",
	"publisher": "Fuse-Gaming",
	"version": "0.0.1",
	"engines": {
		"vscode": "^1.59.0"
	},
	"categories": [
		"Other"
	],
	"activationEvents": [
		"*"
	],
	"main": "./out/extension.js",
	"contributes": {
		"commands": [
			{
				"command": "vs-codex.explain",
				"title": "VS Codex: Explain selected code"
			},
			{
				"command": "vs-codex.complete",
				"title": "VS Codex: Complete code after cursor"
			},
			{
				"command": "vs-codex.change-completion-type",
				"title": "VS Codex: Change completion type"
			},
			{
				"command": "vs-codex.change-model-type",
				"title": "VS Codex: Change Codex model type"
			}
		],
		"keybindings": [
			{
				"command": "vs-codex.complete",
				"key": "ctrl+enter"
			}
		],
		"configuration": {
			"title": "VS Codex",
			"properties": {
				"general.openaiKey": {
					"type": "string",
					"default": "secret key",
					"description": "Open AI secret key"
				},
				"general.maxTokens": {
					"type": "number",
					"default": 256,
					"minimum": 64,
					"maximum": 1024,
					"description": "Maximum number of tokens the GPT model can generate"
				},
				"completions.promptSize": {
					"type": "number",
					"default": 1024,
					"minimum": 128,
					"maximum": 2048,
					"description": "If smart sampling is not available, this number is use to generate a simple prompt"
				},
				"model.temperature": {
					"type": "number",
					"default": 0.2,
					"minimum": 0,
					"maximum": 1,
					"description": "Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer."
				},
				"model.topP": {
					"type": "number",
					"default": 1,
					"minimum": 0,
					"maximum": 1,
					"description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered."
				},
				"model.n": {
					"type": "number",
					"default": 1,
					"minimum": 1,
					"maximum": 10,
					"description": "How many completions to generate for each prompt. (More predictions = more expensive)"
				},
				"model.frequencyPenalty": {
					"type": "number",
					"default": 0,
					"minimum": -2,
					"maximum": 2,
					"description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."
				},
				"model.presencePenalty": {
					"type": "number",
					"default": 0,
					"minimum": -2,
					"maximum": 2,
					"description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."
				},
				"model.bestOf": {
					"type": "number",
					"default": 1,
					"minimum": 1,
					"maximum": 10,
					"description": "Generates best_of completions server-side and returns the \"best\" (the one with the lowest log probability per token). Results cannot be streamed. (More predictions = more expensive)"
				}
			}
		}
	},
	"scripts": {
		"vscode:prepublish": "npm run compile",
		"compile": "tsc -p ./",
		"watch": "tsc -watch -p ./",
		"pretest": "npm run compile && npm run lint",
		"lint": "eslint src --ext ts",
		"test": "node ./out/test/runTest.js"
	},
	"devDependencies": {
		"@types/glob": "^7.1.3",
		"@types/mocha": "^8.2.2",
		"@types/node": "14.x",
		"@typescript-eslint/eslint-plugin": "^4.26.0",
		"@typescript-eslint/parser": "^4.26.0",
		"eslint": "^7.27.0",
		"glob": "^7.1.7",
		"mocha": "^8.4.0",
		"typescript": "^4.3.2",
		"vscode-test": "^1.5.2"
	},
	"dependencies": {
		"axios": "^0.21.1"
	}
}
